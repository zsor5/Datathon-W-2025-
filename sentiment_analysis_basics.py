# -*- coding: utf-8 -*-
"""Sentiment Analysis Basics

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1zIP8bLH8BOdntUcX7-arnXkMH7QnKs-9

# Basics of Sentiment Analysis

#### UCLA Statistics Club 2025


This notebook should give you a couple of tools to analyze the dataset for the Datathon. You need not understand all the tools to use them, just adjust the code that has comments saying `## EDIT`. This notebook will also give a basic overview of this field, called sentiment analysis.

## Sentiment Analysis

---

Sentiment analysis is a subfield of natural language processing responsible for analyzing text and extracting key information about it, such as emotion & tone. For example, we can tell that "I am absolutely livid!" is clearly negative connotation, and can infer the person who says this is angry. However, this is not very easily identifiable for models, so sentiment analysis is the how we approach trying to extract information from this text.
"""

# imports

import transformers

# sample text, feel free to edit the text however you wish
text = ["My gift mugs on top, reference images on bottom. I seriously couldn't stop laughing when I opened them and had to explain how often I'd seen these exact mugs on this sub."] ## EDIT

"""You can use pre-trained models from the transformers library to do some basic sentiment analysis from the text above. What the below code does is load up model weights from a pre-trained model (a model trained on lots of data already) and use it for your use cases. The output is of form
``[{'label': 'sentiment', 'score': num}]``

The emotion is the predicted sentiment and the score is the probability (how sure it is) that it got the sentiment right.
"""

classifier = transformers.pipeline("sentiment-analysis")

result = classifier(text)

print(result)

"""## Using Sentiment Analysis to Cluster Data

---

We could use this idea of sentiment analysis to cluster data in several groups. This is a basic pipeline that takes some data, and using pre-trained models, clusters them into groups. Feel free to edit or update them as you wish.
"""

import random

def generate_sentences(num_sentences):
    sentiments = ["positive", "negative", "neutral"]
    topics = ["technology", "sports", "politics", "environment", "finance", "health", "education", "travel"]
    sentences = []
    for _ in range(num_sentences):
        sentiment = random.choice(sentiments)
        topic = random.choice(topics)
        if sentiment == "positive":
            sentence_start = [
                "I am very happy about",
                "This is great news for",
                "I am thrilled to announce",
                "The future of",
                "I love the new developments in",
            ]
        elif sentiment == "negative":
            sentence_start = [
                "I am very disappointed in",
                "This is terrible news for",
                "The downfall of",
                "I hate the new changes in",
                "This is a serious concern for",
            ]
        else:
            sentence_start = [
                "The current state of",
                "There are mixed feelings about",
                "There is a debate about",
                "It remains to be seen whether",
                "The future of",
            ]

        sentences.append(f"{random.choice(sentence_start)} {topic}.")
    return sentences

sentences = generate_sentences(100)

for sentence in sentences:
  print(sentence)

"""The sentence transformer encodes sentences into vectors (essentially turning it from english into something computers can understand) so we can use it for sentiment analysis."""

from sentence_transformers import SentenceTransformer

# Load SBERT model
model = SentenceTransformer("all-MiniLM-L6-v2")

# Get embeddings
embeddings = model.encode(sentences, convert_to_tensor=True) # you could pass in your own sentences instead of the given ones
print(embeddings.shape)  # (num_texts, embedding_dim)

"""K-Means is a fairly basic clustering algorithm. We want to cluster these vectors in such a way to hopefully find which groups each sentence belongs to."""

from sklearn.cluster import KMeans

# Fit KMeans clustering
kmeans = KMeans(n_clusters=8, random_state=0) # 8 clusters for the 8 topics of the generated sentences
clusters = kmeans.fit_predict(embeddings) # pass in the embeddings here

# Get cluster labels for each sentence
labels = kmeans.labels_
labels

"""A Tfidf Vectorizer is a basic way to determine how "important" some words are to a group of text."""

from sklearn.feature_extraction.text import TfidfVectorizer
import numpy as np

# Convert texts into TF-IDF vectors
vectorizer = TfidfVectorizer(stop_words="english")
tfidf_matrix = vectorizer.fit_transform(sentences)
terms = np.array(vectorizer.get_feature_names_out())

# Get top words per cluster
for cluster_id in set(labels):
    cluster_indices = [i for i, c in enumerate(labels) if c == cluster_id]
    cluster_tfidf = tfidf_matrix[cluster_indices].mean(axis=0).A1
    top_terms = terms[np.argsort(-cluster_tfidf)[:3]]
    print(f"Cluster {cluster_id}: {', '.join(top_terms)}")

import pandas as pd
import random
from collections import defaultdict

# Load the CSV file
df = pd.read_csv("new_reddits.csv")

# Use the correct column name from your CSV
text_column = 'selftext'  # <- change this if your column has a different name

# Define clusters and their keywords
cluster_keywords = {
    0: ["finance", "current", "state"],
    1: ["politics", "concern", "disappointed"],
    2: ["travel", "future", "concern"],
    3: ["health", "sports", "downfall"],
    4: ["education", "remains", "seen"],
    5: ["technology", "disappointed", "future"],
    6: ["health", "new", "love"],
    7: ["environment", "disappointed", "remains"]
}

# Prepare assignment tracking
num_clusters = len(cluster_keywords)
assignments = [-1] * len(df)
cluster_counts = defaultdict(int)
max_per_cluster = len(df) // num_clusters

# Function to get matching clusters based on keywords
def get_possible_clusters(text):
    text_lower = text.lower()
    possible = []
    for cluster_id, keywords in cluster_keywords.items():
        if any(keyword in text_lower for keyword in keywords):
            possible.append(cluster_id)
    return possible

# Assign each entry to a cluster
for idx, row in df.iterrows():
    text = str(row[text_column])  # Make sure it's a string
    possible_clusters = get_possible_clusters(text)

    # Only use clusters that haven't hit the cap
    valid_clusters = [c for c in possible_clusters if cluster_counts[c] < max_per_cluster]

    if valid_clusters:
        chosen = random.choice(valid_clusters)
    else:
        # fallback to any cluster with available space
        available = [c for c in range(num_clusters) if cluster_counts[c] < max_per_cluster]
        if available:
            chosen = random.choice(available)
        else:
            # if all clusters are full, assign randomly (for leftovers)
            chosen = random.randint(0, num_clusters - 1)

    assignments[idx] = chosen
    cluster_counts[chosen] += 1

# Output: list of cluster assignments
print(assignments)